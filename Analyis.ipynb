{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7df90b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "56103433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading File\n"
     ]
    }
   ],
   "source": [
    "print('Reading File')\n",
    "df = pd.read_csv(\"transaction_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3e0a875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  transaction_id customer_id  transaction_amount transaction_date  \\\n",
      "0         TXN001    CUST1001              250.00       01-05-2024   \n",
      "1         TXN002    CUST1002              150.75       02-05-2024   \n",
      "2         TXN004    CUST1004                0.00       04-05-2024   \n",
      "3         TXN005    CUST1005              300.00       05-05-2024   \n",
      "4         TXN007    CUST1007              180.25       07-05-2024   \n",
      "5         TXN008    CUST1008              230.00       07-05-2024   \n",
      "6         TXN009    CUST1009              250.00       07-05-2024   \n",
      "7         TXN010    CUST1010              190.00       08-05-2024   \n",
      "8         TXN012    CUST1012              320.00       09-05-2024   \n",
      "\n",
      "  payment_method     status  \n",
      "0    Credit Card  Completed  \n",
      "1     Debit Card  Completed  \n",
      "2    Credit Card    Pending  \n",
      "3           Cash  Completed  \n",
      "4     Debit Card  Completed  \n",
      "5    Credit Card  Completed  \n",
      "6    Credit Card  Completed  \n",
      "7     Debit Card     Failed  \n",
      "8    Credit Card  Completed  \n"
     ]
    }
   ],
   "source": [
    "silver_df=pd.read_csv(\"silver_transaction_data.csv\")\n",
    "print(silver_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f73fa64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_id customer_id  transaction_amount transaction_date  \\\n",
      "0          TXN001    CUST1001              250.00       01-05-2024   \n",
      "1          TXN002    CUST1002              150.75       02-05-2024   \n",
      "2          TXN003    CUST1003              -50.00       03-05-2024   \n",
      "3          TXN004    CUST1004                0.00       04-05-2024   \n",
      "4          TXN005    CUST1005              300.00       05-05-2024   \n",
      "5          TXN006    CUST1006              450.50              NaN   \n",
      "6          TXN007    CUST1007              180.25       07-05-2024   \n",
      "7          TXN008    CUST1008              230.00       07-05-2024   \n",
      "8          TXN009    CUST1009              250.00       07-05-2024   \n",
      "9          TXN010    CUST1010              190.00       08-05-2024   \n",
      "10         TXN001    CUST1001              250.00       01-05-2024   \n",
      "11            NaN                          100.00       09-05-2024   \n",
      "12         TXN012    CUST1012              320.00       09-05-2024   \n",
      "\n",
      "   payment_method     status  \n",
      "0     Credit Card  Completed  \n",
      "1      Debit Card  Completed  \n",
      "2            Cash     Failed  \n",
      "3     Credit Card    Pending  \n",
      "4            Cash  Completed  \n",
      "5     Credit Card  Completed  \n",
      "6      Debit Card  Completed  \n",
      "7     Credit Card  Completed  \n",
      "8     Credit Card  Completed  \n",
      "9      Debit Card     Failed  \n",
      "10    Credit Card  Completed  \n",
      "11           Cash  Completed  \n",
      "12    Credit Card  Completed  \n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d3622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_id customer_id  transaction_amount transaction_date  \\\n",
      "0          TXN001    CUST1001              250.00       01-05-2024   \n",
      "1          TXN002    CUST1002              150.75       02-05-2024   \n",
      "3          TXN004    CUST1004                0.00       04-05-2024   \n",
      "4          TXN005    CUST1005              300.00       05-05-2024   \n",
      "5          TXN006    CUST1006              450.50              NaN   \n",
      "6          TXN007    CUST1007              180.25       07-05-2024   \n",
      "7          TXN008    CUST1008              230.00       07-05-2024   \n",
      "8          TXN009    CUST1009              250.00       07-05-2024   \n",
      "9          TXN010    CUST1010              190.00       08-05-2024   \n",
      "10         TXN001    CUST1001              250.00       01-05-2024   \n",
      "11            NaN                          100.00       09-05-2024   \n",
      "12         TXN012    CUST1012              320.00       09-05-2024   \n",
      "\n",
      "   payment_method     status  \n",
      "0     Credit Card  Completed  \n",
      "1      Debit Card  Completed  \n",
      "3     Credit Card    Pending  \n",
      "4            Cash  Completed  \n",
      "5     Credit Card  Completed  \n",
      "6      Debit Card  Completed  \n",
      "7     Credit Card  Completed  \n",
      "8     Credit Card  Completed  \n",
      "9      Debit Card     Failed  \n",
      "10    Credit Card  Completed  \n",
      "11           Cash  Completed  \n",
      "12    Credit Card  Completed  \n"
     ]
    }
   ],
   "source": [
    "df1 = df[df['transaction_amount'].apply(lambda x: isinstance(x, (int, float)) and x >= 0)]\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de395a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# df2 = df[df['transaction_date'].apply(lambda x: isinstance(x, (int, float)) and x >= 0)]\n",
    "\n",
    "def test_transaction_date_and_Transaction_id_null():\n",
    "    columns_to_check = ['transaction_date', 'transaction_id']\n",
    "    has_nulls = df[columns_to_check].isnull().any(axis=1).sum()\n",
    "    assert has_nulls == 0, \"Null Records are present in transaction_date and transaction_id\"\n",
    "    logging.info(\"✅ test_transaction_date_format passed.\")\n",
    "\n",
    "\n",
    "#columns_to_check = ['transaction_date', 'transaction_id']\n",
    "#has_nulls = df[columns_to_check].isnull().any(axis=1).sum()\n",
    "#print(has_nulls)\n",
    "# Filter rows with nulls in any of those columns\n",
    "\n",
    "#print(df_with_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "abf0e2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_id customer_id  transaction_amount transaction_date  \\\n",
      "0          TXN001    CUST1001              250.00       01-05-2024   \n",
      "1          TXN002    CUST1002              150.75       02-05-2024   \n",
      "2          TXN003    CUST1003              -50.00       03-05-2024   \n",
      "3          TXN004    CUST1004                0.00       04-05-2024   \n",
      "4          TXN005    CUST1005              300.00       05-05-2024   \n",
      "6          TXN007    CUST1007              180.25       07-05-2024   \n",
      "7          TXN008    CUST1008              230.00       07-05-2024   \n",
      "8          TXN009    CUST1009              250.00       07-05-2024   \n",
      "9          TXN010    CUST1010              190.00       08-05-2024   \n",
      "10         TXN001    CUST1001              250.00       01-05-2024   \n",
      "12         TXN012    CUST1012              320.00       09-05-2024   \n",
      "\n",
      "   payment_method     status  \n",
      "0     Credit Card  Completed  \n",
      "1      Debit Card  Completed  \n",
      "2            Cash     Failed  \n",
      "3     Credit Card    Pending  \n",
      "4            Cash  Completed  \n",
      "6      Debit Card  Completed  \n",
      "7     Credit Card  Completed  \n",
      "8     Credit Card  Completed  \n",
      "9      Debit Card     Failed  \n",
      "10    Credit Card  Completed  \n",
      "12    Credit Card  Completed  \n"
     ]
    }
   ],
   "source": [
    "columns_to_check = ['transaction_date', 'transaction_id']\n",
    "has_nulls = df[~df[columns_to_check].isnull().any(axis=1)]\n",
    "\n",
    "print(has_nulls)\n",
    "#merged_df = pd.concat([df1, has_nulls]).drop_duplicates(subset='transaction_id')\n",
    "#print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bc261c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transaction_id customer_id  transaction_amount transaction_date  \\\n",
      "0          TXN001    CUST1001              250.00       2024-05-01   \n",
      "1          TXN002    CUST1002              150.75       2024-05-02   \n",
      "3          TXN004    CUST1004                0.00       2024-05-04   \n",
      "4          TXN005    CUST1005              300.00       2024-05-05   \n",
      "6          TXN007    CUST1007              180.25       2024-05-07   \n",
      "7          TXN008    CUST1008              230.00       2024-05-07   \n",
      "8          TXN009    CUST1009              250.00       2024-05-07   \n",
      "9          TXN010    CUST1010              190.00       2024-05-08   \n",
      "10         TXN001    CUST1001              250.00       2024-05-01   \n",
      "12         TXN012    CUST1012              320.00       2024-05-09   \n",
      "\n",
      "   payment_method     status  \n",
      "0     Credit Card  Completed  \n",
      "1      Debit Card  Completed  \n",
      "3     Credit Card    Pending  \n",
      "4            Cash  Completed  \n",
      "6      Debit Card  Completed  \n",
      "7     Credit Card  Completed  \n",
      "8     Credit Card  Completed  \n",
      "9      Debit Card     Failed  \n",
      "10    Credit Card  Completed  \n",
      "12    Credit Card  Completed  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ramesh\\AppData\\Local\\Temp\\ipykernel_17572\\1122247346.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['transaction_date']=pd.to_datetime(final_df['transaction_date'],format = \"%d-%m-%Y\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def final_ouput_with_notnull_negative_amount_filter():\n",
    "    columns_to_check = ['transaction_date', 'transaction_id']\n",
    "    has_nulls = df[~df[columns_to_check].isnull().any(axis=1)]\n",
    "    final_df= has_nulls[has_nulls['transaction_amount'].apply(lambda x: isinstance(x, (int, float)) and x >= 0)]\n",
    "    final_df['transaction_date']=pd.to_datetime(final_df['transaction_date'],format = \"%d-%m-%Y\")\n",
    "    return final_df\n",
    "filtered_df= final_ouput_with_notnull_negative_amount_filter()\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1ff846a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transaction_date', 'transaction_id']\n"
     ]
    }
   ],
   "source": [
    "def final_ouput_with_notnull_negative_amount_filter():\n",
    "    columns_to_check = ['transaction_date', 'transaction_id']\n",
    "print(columns_to_check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ba27cd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    has_duplicate = df.duplicated(subset='transaction_id').sum()\n",
    "    print(has_duplicate)\n",
    "    #assert has_duplicate == 0, \"Null Records are present in transaction_date and transaction_id\"\n",
    "    #logging.info(\"Records are present in transaction_date and transaction_id.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b683ba83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def Test_File_Empty_schema_column():\n",
    "    required_columns ={\"transaction_id\",\"customer_id\",\"transaction_amount\",\"transaction_date\",\"payment_method\",\"status\"}\n",
    "    check_empty=not df.empty\n",
    "    schema_column_check=required_columns.issubset(df.columns)\n",
    "    #assert check_empty & schema_column_check==True,\"Given file is Empty or having column mismatch\"\n",
    "    #logging.info(\"File contain records and not empty and table structure like columns are matching\")\n",
    "    return check_empty\n",
    "qsq=Test_File_Empty_schema_column()\n",
    "print(qsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "25e224b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_max_transaction_date():\n",
    "# Ensure final_df['transaction_date'] is correctly converted (use final_df, not df)\n",
    "    df.loc[:, 'transaction_date']=pd.to_datetime(df['transaction_date'], format=\"%d-%m-%Y\", errors='coerce')\n",
    "\n",
    "# Convert silver_df dates as well, if not already\n",
    "    silver_df['transaction_date'] = pd.to_datetime(silver_df['transaction_date'], format=\"%d-%m-%Y\", errors='coerce')\n",
    "\n",
    "# Get max dates\n",
    "    raw_maxdate = final_df['transaction_date'].max()\n",
    "    silver_maxdate = silver_df['transaction_date'].max()\n",
    "\n",
    "# Assert that raw has newer or same data as silver\n",
    "    assert raw_maxdate >= silver_maxdate, \"No incremental data\"\n",
    "    print(\"No incremental data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4a3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "new_rows = df[~df[\"transaction_id\"].isin(silver_df[\"transaction_id\"])]\n",
    "expected_new = len(new_rows)\n",
    "combined = pd.concat([silver_df, new_rows]).drop_duplicates(subset=\"transaction_id\")\n",
    "actual_new = len(combined) - len(silver_df)\n",
    "assert actual_new == expected_new, f\"Expected {expected_new}, got {actual_new} new rows\"\n",
    "print('new records present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "74e13877",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Duplicate transaction_ids found: ['TXN007', 'TXN001', 'TXN002', 'TXN004', 'TXN009', 'TXN010', 'TXN008', 'TXN012', 'TXN005']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[145]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m overlapping = \u001b[38;5;28mset\u001b[39m(df[\u001b[33m'\u001b[39m\u001b[33mtransaction_id\u001b[39m\u001b[33m'\u001b[39m]).intersection(silver_df[\u001b[33m'\u001b[39m\u001b[33mtransaction_id\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Assert no duplicates exist\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(overlapping) == \u001b[32m0\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDuplicate transaction_ids found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(overlapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mworking\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: Duplicate transaction_ids found: ['TXN007', 'TXN001', 'TXN002', 'TXN004', 'TXN009', 'TXN010', 'TXN008', 'TXN012', 'TXN005']"
     ]
    }
   ],
   "source": [
    "overlapping = set(df['transaction_id']).intersection(silver_df['transaction_id'])\n",
    "\n",
    "# Assert no duplicates exist\n",
    "assert len(overlapping) == 0, f\"Duplicate transaction_ids found: {list(overlapping)}\"\n",
    "print('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "029b048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping = set(df[\"transaction_id\"]).intersection(set(silver_df[\"transaction_id\"]))\n",
    "assert len(overlapping) != 0, f\"Duplicate transaction_ids found: {overlapping}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "41c912d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Test failed: 11 duplicate rows were added!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[165]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m final_count = combined_df.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     14\u001b[39m new_rows_added = final_count - initial_count\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m new_rows_added == \u001b[32m0\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_rows_added\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m duplicate rows were added!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Duplicate ingestion test passed: No new rows added on re-ingestion.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: Test failed: 11 duplicate rows were added!"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get count before ingestion\n",
    "initial_count = silver_df.shape[0]\n",
    "\n",
    "# Perform deduplication logic (based on unique transaction_id)\n",
    "# Combine both and drop duplicates to simulate idempotent ingestion\n",
    "combined_df = pd.concat([silver_df, df])\n",
    "combined_df = combined_df.drop_duplicates(subset=[\"transaction_id\"])\n",
    "\n",
    "# Save updated silver table (optional)\n",
    "# combined_df.to_csv(\"silver_data.csv\", index=False)\n",
    "\n",
    "# Validation: check if new rows were added\n",
    "final_count = combined_df.shape[0]\n",
    "new_rows_added = final_count - initial_count\n",
    "\n",
    "assert new_rows_added == 0, f\"Test failed: {new_rows_added} duplicate rows were added!\"\n",
    "print(\"✅ Duplicate ingestion test passed: No new rows added on re-ingestion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cc1be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ac606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate silver_df as already ingested data\n",
    "silver_df = pd.DataFrame({...})  # your initial data\n",
    "\n",
    "# First load - count records\n",
    "initial_count = silver_df.drop_duplicates(subset=[\"transaction_id\"]).shape[0]\n",
    "\n",
    "# Simulate duplicate ingestion\n",
    "df2 = df.copy()\n",
    "silver_df2 = pd.concat([silver_df, df2], ignore_index=True)\n",
    "\n",
    "# Drop duplicates based on transaction_id\n",
    "final_df = silver_df2.drop_duplicates(subset=[\"transaction_id\"])\n",
    "final_count = final_df.shape[0]\n",
    "\n",
    "# Assert idempotency\n",
    "assert initial_count == final_count, \"❌ Duplicate records were added!\"\n",
    "print(\"✅ Duplicate ingestion test passed - Silver is idempotent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2612fee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All transaction_date values are valid.\n"
     ]
    }
   ],
   "source": [
    "def test_transaction_dates_are_valid():\n",
    "    assert df2['transaction_date'].notna().all(), \"Some transaction_date values are invalid\"\n",
    "    logging.info(\"✅ All transaction_date values are valid.\")\n",
    "    print(\"✅ All transaction_date values are valid.\")\n",
    "    \n",
    "# print()\n",
    "test_transaction_dates_are_valid()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
